{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MedMNIST'...\n",
      "remote: Enumerating objects: 156, done.\u001b[K\n",
      "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
      "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
      "remote: Total 156 (delta 84), reused 87 (delta 37), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (156/156), 2.29 MiB | 5.72 MiB/s, done.\n",
      "Resolving deltas: 100% (84/84), done.\n",
      "Processing ./MedMNIST\n",
      "Collecting torch\n",
      "  Using cached torch-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (735.5 MB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from MedMNIST==0.2.2) (1.18.5)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from MedMNIST==0.2.2) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from MedMNIST==0.2.2) (0.23.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from MedMNIST==0.2.2) (4.42.1)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from pandas->MedMNIST==0.2.2) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from pandas->MedMNIST==0.2.2) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from scikit-learn->MedMNIST==0.2.2) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from scikit-learn->MedMNIST==0.2.2) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from scikit-learn->MedMNIST==0.2.2) (0.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->MedMNIST==0.2.2) (1.14.0)\n",
      "Building wheels for collected packages: MedMNIST\n",
      "  Building wheel for MedMNIST (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for MedMNIST: filename=MedMNIST-0.2.2-py3-none-any.whl size=13970 sha256=e45021879b5281e9c4d8d2b0dfdd80536983aed6b02359115e5de809b3ecd5a7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pi0k0xps/wheels/49/0c/0d/6ea2e235481c92ab5ba6b458810dae2b0aae3ffdb968abf5a7\n",
      "Successfully built MedMNIST\n",
      "\u001b[31mERROR: Error checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 517, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/s3transfer-0.3.3.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: dataclasses, typing-extensions, torch, MedMNIST\n",
      "Successfully installed MedMNIST-0.2.2 dataclasses-0.8 torch-1.8.0 typing-extensions-3.7.4.3\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (17.3 MB)\n",
      "Requirement already satisfied: seaborn in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.10.0)\n",
      "Collecting nibabel\n",
      "  Using cached nibabel-3.2.1-py3-none-any.whl (3.3 MB)\n",
      "Requirement already satisfied: torch==1.8.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from torchvision->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from torchvision->-r requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from torchvision->-r requirements.txt (line 1)) (1.18.5)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from seaborn->-r requirements.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from seaborn->-r requirements.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from seaborn->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from nibabel->-r requirements.txt (line 3)) (20.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from torch==1.8.0->torchvision->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from torch==1.8.0->torchvision->-r requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn->-r requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from pandas>=0.22.0->seaborn->-r requirements.txt (line 2)) (2019.3)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from packaging>=14.3->nibabel->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn->-r requirements.txt (line 2)) (45.2.0.post20200210)\n",
      "\u001b[31mERROR: Error checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 517, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/s3transfer-0.3.3.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: torchvision, nibabel\n",
      "Successfully installed nibabel-3.2.1 torchvision-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# MedMNIST kütüphanesini indirip sistemimizze yüklüyoruz\n",
    "! git clone https://github.com/MedMNIST/MedMNIST\n",
    "! pip install MedMNIST/ && rm -rf MedMNIST\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset hakkında ayrıntılı bilgi için [MedMNIST](https://medmnist.github.io/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import medmnist\n",
    "from medmnist.dataset import OrganMNISTAxial\n",
    "from medmnist.info import INFO\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils import plot_cm, show_random_images, show_train_history, to_one_hot, visualize_sample_data_gif, show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/record/4269852/files/organmnist_axial.npz?download=1 to data/organmnist_axial.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992ddc0a09754317a21f0153fe239a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38247903.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OrganMNIST_Axial: A dataset based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into grey scale with a abdominal window; we then crop 2D images from the center slices of the 3D bounding boxes in axial views (planes). The images are resized into 1 x 28 x 28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.\n",
      "\n",
      "Class Names:\n",
      "0 : bladder\n",
      "1 : femur-left\n",
      "2 : femur-right\n",
      "3 : heart\n",
      "4 : kidney-left\n",
      "5 : kidney-right\n",
      "6 : liver\n",
      "7 : lung-left\n",
      "8 : lung-right\n",
      "9 : pancreas\n",
      "10 : spleen\n"
     ]
    }
   ],
   "source": [
    "# Dataseti indiriyoruz\n",
    "dataset_name = 'organmnist_axial'\n",
    "data_path = \"data/\"\n",
    "OrganMNISTAxial(root=data_path, transform=None, download=True)\n",
    "\n",
    "info = INFO[dataset_name]\n",
    "class_names = info[\"label\"]\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "print(info['description'])\n",
    "print(f'\\nClass Names:')\n",
    "for idx, c_name in class_names.items():\n",
    "    print(idx,\":\", c_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indirdiğimiz npz formatında olan dataseti okuyoruz.\n",
    "data_npz = np.load(os.path.join(data_path, dataset_name + \".npz\"))\n",
    "\n",
    "data = {}\n",
    "for key in data_npz.keys():\n",
    "    data[key] = data_npz[key]\n",
    "    \n",
    "train_images, train_labels = data['train_images'], data['train_labels']\n",
    "val_images, val_labels = data['val_images'], data['val_labels']\n",
    "\n",
    "train_images.shape, train_labels.shape, val_images.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(train_images, train_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originial data\n",
    "visualize_sample_data_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetimizi normalize ediyoruz\n",
    "min_ = train_images.min()\n",
    "max_ = train_images.max()\n",
    "\n",
    "def minmax(images):\n",
    "    global min_, max_\n",
    "    return (images - min_) / (max_ - min_)\n",
    "\n",
    "print(f\"Before -> train min: {train_images.min():.2f} train mean: {train_images.mean():.2f} train max: {train_images.max():.2f}\")\n",
    "train_images = minmax(train_images)\n",
    "val_images = minmax(val_images)\n",
    "\n",
    "print(f\"After  -> train min: {train_images.min():.2f} train mean: {train_images.mean():.2f} train max: {train_images.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding\n",
    "<br>\n",
    "![title](https://miro.medium.com/max/700/1*ggtP4a5YaRx6l09KQaYOnw.png)\n",
    "<br>\n",
    "https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_one_hot(train_labels, num_classes=n_classes)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN modelimizi oluşturuyoruz\n",
    "\n",
    "ann = tf.keras.models.Sequential()\n",
    "ann.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
    "ann.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "ann.add(tf.keras.layers.Dropout(0.2))\n",
    "ann.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "ann.add(tf.keras.layers.Dropout(0.2))\n",
    "ann.add(tf.keras.layers.Dense(n_classes, activation=\"softmax\"))\n",
    "\n",
    "ann.compile(optimizer=\"adam\",\n",
    "            loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "ann.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function\n",
    "\n",
    "<div>\n",
    "<img src=\"https://ljvmiranda921.github.io/assets/png/cs231n-ann/softmax.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ann.fit(train_images, to_one_hot(train_labels, num_classes=n_classes),\n",
    "                  validation_data=(val_images, to_one_hot(val_labels, num_classes=n_classes)),\n",
    "                  epochs=20)\n",
    "show_train_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = ann.predict(val_images)\n",
    "val_preds = np.argmax(probas, axis=-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(val_images, val_labels, class_names, n=4, preds=val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(np.squeeze(val_labels), np.squeeze(val_preds), class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding=\"same\", input_shape=(28,28,1)))\n",
    "cnn.add(tf.keras.layers.MaxPool2D((2, 2)))\n",
    "cnn.add(tf.keras.layers.Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dropout(0.2))\n",
    "cnn.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "cnn.compile(optimizer=\"adam\",\n",
    "            loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_history = cnn.fit(np.expand_dims(train_images, axis=-1), \n",
    "        to_one_hot(train_labels, num_classes=n_classes),\n",
    "        validation_data=(np.expand_dims(val_images, axis=-1), tf.keras.utils.to_categorical(val_labels, num_classes=n_classes)),\n",
    "        epochs=20)\n",
    "show_train_history(cnn_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = cnn.predict(np.expand_dims(val_images,axis=-1))\n",
    "val_preds = np.argmax(probas, axis=-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(val_images, val_labels, class_names, n=4, preds=val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(np.squeeze(val_labels), np.squeeze(val_preds), class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.evaluate(np.expand_dims(val_images, axis=-1), to_one_hot(val_labels,n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.evaluate(np.expand_dims(test_images, axis=-1), to_one_hot(test_labels,n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = cnn.predict(np.expand_dims(test_images,axis=-1))\n",
    "val_preds = np.argmax(probas, axis=-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(test_images, test_labels, class_names, n=4, preds=val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
